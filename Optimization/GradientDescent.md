# 0. 引言

在机器学习及深度学习领域，更新模型参数时涉及到许多最优化方法，本文将介绍梯度下降法。

# 1. 梯度下降

## 1.1 概念

梯度下降法（gradient descent）或最速下降法（steepest descent），求解无约束最优化问题的一种常用方法。梯度下降法是一种迭代算法，每一步求解目标函数的梯度向量。

## 1.2 原理

假设$f(x)$是需要优化的损失函数，在$\pmb{R}^n$上具有一阶连续偏导数。要求解的无约束最优化问题：
$$
\underset{x \in \pmb{R^n}}{min} \ \    f(x) \tag1
$$

选取适当的初值$x^{(0)}$，不断迭代，更新$x$值，进行目标函数的极小化，直到收敛。由于负梯度方向是函数值下降最快的方向（详细见第二章），在迭代的每一步，以负梯度方向更新$x$。

## 1.3 算法

输入：目标函数$f(x)$，梯度函数$g(x) = \nabla f(x)$，计算精度$\varepsilon $；

输出：$f(x)$的极小值点$x^*$；

(1) 取初始值$x^{(0)} \in \pmb R^n$，置$k=0$；

(2) 计算$f(x)$；

(3) 计算梯度$g_k = g(x^{(k)})$；当$||g_k|| < \varepsilon$时，停止迭代，令$x^* = x^{(k)}$；否则，令$p_k = -g(x^{(k)})$

，求$\lambda_k$，使得
$$
f(x^{(k)} + \lambda_k p_k) = \underset{\lambda \geq 0}{min}f(x^{(k)} + \lambda p_k)
$$
(4) 置$x^{(k+1)} = x^{(k)} + \lambda_k p_k$，计算$f(x^{(k+1)})$；当$||f(x^{(k+1)})- f(x^{(k)})|| < \varepsilon$ 或$||x^{(k+1)}- x^{(k)}|| < \varepsilon $ 时，停止迭代，令$x^* = x^{(k+1)}$；否则，置$k = k + 1$，转(3)。

## 1.4 特点

* 当目标函数是凸函数时，梯度下降法的解是全局最优解。否则，可能是局部最优解。
* 梯度下降法的收敛速度未必很快。

# 2. 负梯度方向

## 2.1 导数



## 2.2 偏导数



## 2.3 方向导数



## 2.4 梯度





# 3. 参考文献

[1] 统计学习方法（第二版），李航

[2] 高等数学，统计大学版

