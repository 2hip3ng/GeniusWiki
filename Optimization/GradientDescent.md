# 0. 引言

在机器学习及深度学习领域，更新模型参数时涉及到许多最优化方法，本文将介绍梯度下降法。

# 1. 梯度下降

## 1.1 概念

梯度下降法（gradient descent）或最速下降法（steepest descent），求解无约束最优化问题的一种常用方法。梯度下降法是一种迭代算法，每一步求解目标函数的梯度向量。

## 1.2 原理

假设$f(x)$是需要优化的损失函数，在$\pmb{R}^n$上具有一阶连续偏导数。要求解的无约束最优化问题：
$$
\underset{x \in \pmb{R^n}}{min} \ \    f(x) \tag1
$$

选取适当的初值$x^{(0)}$，不断迭代，更新$x$值，进行目标函数的极小化，直到收敛。由于负梯度方向是函数值下降最快的方向（详细见第二节），在迭代的每一步，以负梯度方向更新$x$。

## 1.3 算法

输入：目标函数$f(x)$，梯度函数$g(x) = \nabla f(x)$，计算精度$\varepsilon $；

输出：$f(x)$的极小值点$x^*$；

(1) 取初始值$x^{(0)} \in \pmb R^n$，置$k=0$；

(2) 计算$f(x)$；

(3) 计算梯度$g_k = g(x^{(k)})$；当$||g_k|| < \varepsilon$时，停止迭代，令$x^* = x^{(k)}$；否则，令$p_k = -g(x^{(k)})$

，求$\lambda_k$，使得
$$
f(x^{(k)} + \lambda_k p_k) = \underset{\lambda \geq 0}{min}f(x^{(k)} + \lambda p_k)  \tag2
$$
(4) 置$x^{(k+1)} = x^{(k)} + \lambda_k p_k$，计算$f(x^{(k+1)})$；当$||f(x^{(k+1)})- f(x^{(k)})|| < \varepsilon$ 或$||x^{(k+1)}- x^{(k)}|| < \varepsilon $ 时，停止迭代，令$x^* = x^{(k+1)}$；否则，置$k = k + 1$，转(3)。

## 1.4 特点

* 当目标函数是凸函数时，梯度下降法的解是全局最优解。否则，可能是局部最优解。
* 梯度下降法的收敛速度未必很快。

# 2. 负梯度方向

上文提到，在梯度下降法中，负梯度方向时下降最快的方向，本节对其进行详细解释。

## 2.1 导数

先来看一个物理学上的引子，假设一个质点沿着直线运动，质点在时刻$t$的位置为s，即：
$$
s  = f(t) \tag 3
$$
取时刻$t_0$到$t$的一个时间间隔，质点从位置$s_0 = f(t_0)$移动到$s = f(t)$，比值
$$
\frac{s-s_0}{t-t_0} = \frac{f(t) - f(t_0)}{t-t_0} \tag4
$$
公式(4)表示了质点在时刻$t_0$到$t$的时间间隔里的平均速度。取$t \rightarrow t_0$，取公式(4)极限，如果极限存在，设为$v$，即：
$$
v = \lim_{t\rightarrow t_0}  \frac{f(t) - f(t_0)}{t-t_0} \tag5
$$
这时，$v$为质点在时刻$t_0$的瞬时速度，描述了质点在时刻$t_0$的位置变化“快慢”。



**导数定义：$f(x)$为某一函数，当因变量$x$在$x_0$处取得增量$\Delta x$ ，因变量取得增量$\Delta y = f(x+x_0) - f(x)$。如果当$\Delta x \rightarrow 0$时，$\frac{\Delta y}{\Delta x}$的极限存在，那么称这个极限为函数$y = f(x)$在点$x_0$处的导数，记为$f'(x_0)$。**
$$
f'(x_0) = \lim_{t \rightarrow t_0} \frac{\Delta y}{\Delta x} = \lim_{t \rightarrow t_0} \frac{f(x_0+\Delta x) - f(x_0)}{\Delta x} \tag6
$$
在质点运动中，导数描述了质点位置变化的“快慢”；相应的，数学上描述了函数的变化率，反映了因变量随着自变量变化而**变化的快慢程度**。

## 2.2 偏导数







## 2.3 方向导数



## 2.4 梯度





# 3. 参考文献

[1] 统计学习方法（第二版），李航

[2] 高等数学，统计大学版

