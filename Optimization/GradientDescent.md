# 0. 引言

在机器学习及深度学习领域，更新模型参数时涉及到许多最优化方法，本文将介绍梯度下降法。

# 1. 梯度下降

## 1.1 概念

梯度下降法（gradient descent）或最速下降法（steepest descent），求解无约束最优化问题的一种常用方法。梯度下降法是一种迭代算法，每一步求解目标函数的梯度向量。

## 1.2 原理

假设$f(x)$是需要优化的损失函数，在$\pmb{R}^n$上具有一阶连续偏导数。要求解的无约束最优化问题：
$$
\underset{x \in \pmb{R^n}}{min} \ \    f(x) \tag1
$$

选取适当的初值$x^{(0)}$，不断迭代，更新$x$值，进行目标函数的极小化，直到收敛。由于负梯度方向是函数值下降最快的方向（详细见第二节），在迭代的每一步，以负梯度方向更新$x$。

## 1.3 算法

输入：目标函数$f(x)$，梯度函数$g(x) = \nabla f(x)$，计算精度$\varepsilon $；

输出：$f(x)$的极小值点$x^*$；

(1) 取初始值$x^{(0)} \in \pmb R^n$，置$k=0$；

(2) 计算$f(x)$；

(3) 计算梯度$g_k = g(x^{(k)})$；当$||g_k|| < \varepsilon$时，停止迭代，令$x^* = x^{(k)}$；否则，令$p_k = -g(x^{(k)})$

，求$\lambda_k$，使得
$$
f(x^{(k)} + \lambda_k p_k) = \underset{\lambda \geq 0}{min}f(x^{(k)} + \lambda p_k)  \tag2
$$
(4) 置$x^{(k+1)} = x^{(k)} + \lambda_k p_k$，计算$f(x^{(k+1)})$；当$||f(x^{(k+1)})- f(x^{(k)})|| < \varepsilon$ 或$||x^{(k+1)}- x^{(k)}|| < \varepsilon $ 时，停止迭代，令$x^* = x^{(k+1)}$；否则，置$k = k + 1$，转(3)。

## 1.4 特点

* 当目标函数是凸函数时，梯度下降法的解是全局最优解。否则，可能是局部最优解。
* 梯度下降法的收敛速度未必很快。

# 2. 负梯度方向

上文提到，在梯度下降法中，负梯度方向时下降最快的方向，本节对其进行详细解释。

## 2.1 导数

先来看一个物理学上的引子，假设一个质点沿着直线运动，质点在时刻$t$的位置为s，即：
$$
s  = f(t) \tag 3
$$
取时刻$t_0$到$t$的一个时间间隔，质点从位置$s_0 = f(t_0)$移动到$s = f(t)$，比值
$$
\frac{s-s_0}{t-t_0} = \frac{f(t) - f(t_0)}{t-t_0} \tag4
$$
公式(4)表示了质点在时刻$t_0$到$t$的时间间隔里的平均速度。取$t \rightarrow t_0$，取公式(4)极限，如果极限存在，设为$v$，即：
$$
v = \lim_{t\rightarrow t_0}  \frac{f(t) - f(t_0)}{t-t_0} \tag5
$$
这时，$v$为质点在时刻$t_0$的瞬时速度，描述了质点在时刻$t_0$的位置变化“快慢”。



**导数定义：设$f(x)$为某一函数，当因变量$x$在$x_0$处取得增量$\Delta x$ ，因变量取得增量$\Delta y = f(x+x_0) - f(x)$。如果当$\Delta x \rightarrow 0$时，$\frac{\Delta y}{\Delta x}$的极限存在，那么称这个极限为函数$y = f(x)$在点$x_0$处的导数，记为$f'(x_0)$。**
$$
f'(x_0) = \lim_{t \rightarrow t_0} \frac{\Delta y}{\Delta x} = \lim_{t \rightarrow t_0} \frac{f(x_0+\Delta x) - f(x_0)}{\Delta x} \tag6
$$
在质点运动中，导数描述了质点位置变化的“快慢”；相应的，导数在数学上描述了函数的变化率，反映了因变量随着自变量变化而**变化的快慢程度**。

## 2.2 偏导数

**偏导数定义：设$z = f(x, y)$为某一多元函数，当$y$固定在$y_0$，而$x$在$x_0$处有增量$\Delta x$，相应的函数增量为$f(x_0+\Delta x, y_0) - f(x_0, y_0)$，如果极限**
$$
\lim_{\Delta x \rightarrow 0} \frac{f(x_0+\Delta x, y_0) - f(x_0, y_0)}{\Delta x} \tag7
$$
**存在，那么称此极限为函数$f(x, y)$在点$(x_0, y_0)$处对$x$的偏导数。类似的，可以得到函数$f(x, y)$在点$(x_0, y_0)$处对$y$的偏导数。记为：**
$$
f_x(x_0, y_0) =  \lim_{\Delta x \rightarrow 0} \frac{f(x_0+\Delta x, y_0) - f(x_0, y_0)}{\Delta x} \tag8
$$

$$
f_y(x_0, y_0) =  \lim_{\Delta y \rightarrow 0} \frac{f(x_0, y_0 + \Delta y) - f(x_0, y_0)}{\Delta y} \tag9
$$

类比于导数的意义，偏导数在数学上描述了多元函数的在各个坐标轴上的**变化的快慢程度**。

## 

## 2.3 方向导数





## 2.4 梯度





# 3. 参考文献

[1] 统计学习方法（第二版），李航

[2] 高等数学，统计大学版

